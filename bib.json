[
  {"id":"brownLanguageModelsAre2020","abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.","accessed":{"date-parts":[[2023,4,16]]},"author":[{"family":"Brown","given":"Tom B."},{"family":"Mann","given":"Benjamin"},{"family":"Ryder","given":"Nick"},{"family":"Subbiah","given":"Melanie"},{"family":"Kaplan","given":"Jared"},{"family":"Dhariwal","given":"Prafulla"},{"family":"Neelakantan","given":"Arvind"},{"family":"Shyam","given":"Pranav"},{"family":"Sastry","given":"Girish"},{"family":"Askell","given":"Amanda"},{"family":"Agarwal","given":"Sandhini"},{"family":"Herbert-Voss","given":"Ariel"},{"family":"Krueger","given":"Gretchen"},{"family":"Henighan","given":"Tom"},{"family":"Child","given":"Rewon"},{"family":"Ramesh","given":"Aditya"},{"family":"Ziegler","given":"Daniel M."},{"family":"Wu","given":"Jeffrey"},{"family":"Winter","given":"Clemens"},{"family":"Hesse","given":"Christopher"},{"family":"Chen","given":"Mark"},{"family":"Sigler","given":"Eric"},{"family":"Litwin","given":"Mateusz"},{"family":"Gray","given":"Scott"},{"family":"Chess","given":"Benjamin"},{"family":"Clark","given":"Jack"},{"family":"Berner","given":"Christopher"},{"family":"McCandlish","given":"Sam"},{"family":"Radford","given":"Alec"},{"family":"Sutskever","given":"Ilya"},{"family":"Amodei","given":"Dario"}],"citation-key":"brownLanguageModelsAre2020","DOI":"10.48550/arXiv.2005.14165","issued":{"date-parts":[[2020,7,22]]},"number":"arXiv:2005.14165","publisher":"arXiv","source":"arXiv.org","title":"Language Models are Few-Shot Learners","type":"article","URL":"http://arxiv.org/abs/2005.14165"},
  {"id":"ConventionalCommits","abstract":"A specification for adding human and machine readable meaning to commit messages","accessed":{"date-parts":[[2023,4,16]]},"citation-key":"ConventionalCommits","container-title":"Conventional Commits","title":"Conventional Commits","type":"webpage","URL":"https://www.conventionalcommits.org/en/v1.0.0/"},
  {"id":"dcyouDcyouResume2022","abstract":"My resume by using NuxtJS (VueJS), Element and some useful tools - Commits Â· dcyou/resume","author":[{"literal":"dcyou"},{"literal":"chapeupreto"}],"citation-key":"dcyouDcyouResume2022","genre":"JavaScript","issued":{"date-parts":[[2022,11,8]]},"license":"MIT","title":"dcyou/resume","title-short":"resume","type":"software","URL":"https://github.com/dcyou/resume"},
  {"id":"fleschHowWritePlain2016","accessed":{"date-parts":[[2023,4,19]]},"author":[{"family":"Flesch","given":"Rudolf"}],"citation-key":"fleschHowWritePlain2016","genre":"Archive","issued":{"date-parts":[[2016,7,12]]},"title":"How to Write Plain English","type":"webpage","URL":"https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml"},
  {"id":"honnibalSpaCyIndustrialstrengthNatural2020","abstract":"ðŸ’« Industrial-strength Natural Language Processing (NLP) in Python","accessed":{"date-parts":[[2023,4,19]]},"author":[{"family":"Honnibal","given":"Matthew"},{"family":"Montani","given":"Ines"},{"family":"Van Landeghem","given":"Sofie"},{"family":"Boyd","given":"Adriane"}],"citation-key":"honnibalSpaCyIndustrialstrengthNatural2020","DOI":"10.5281/zenodo.1212303","genre":"Python","issued":{"date-parts":[[2020]]},"license":"MIT","original-date":{"date-parts":[[2014,7,3]]},"source":"GitHub","title":"spaCy: Industrial-strength Natural Language Processing in Python","title-short":"spaCy","type":"software","URL":"https://github.com/explosion/spaCy"},
  {"id":"huChatGPTSetsRecord2023","abstract":"ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.","accessed":{"date-parts":[[2023,4,2]]},"author":[{"family":"Hu","given":"Krystal"}],"citation-key":"huChatGPTSetsRecord2023","container-title":"Reuters","issued":{"date-parts":[[2023,2,2]]},"language":"en","section":"Technology","source":"www.reuters.com","title":"ChatGPT sets record for fastest-growing user base - analyst note","type":"article-newspaper","URL":"https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"},
  {"id":"IntroducingChatGPT","abstract":"Weâ€™ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.","accessed":{"date-parts":[[2023,4,2]]},"citation-key":"IntroducingChatGPT","language":"en-US","title":"Introducing ChatGPT","type":"webpage","URL":"https://openai.com/blog/chatgpt"},
  {"id":"LinguisticFeaturesSpaCy","abstract":"spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.","accessed":{"date-parts":[[2023,4,19]]},"citation-key":"LinguisticFeaturesSpaCy","container-title":"Linguistic Features","language":"en","title":"Linguistic Features - spaCy Usage Documentation","type":"webpage","URL":"https://spacy.io/usage/linguistic-features#pos-tagging"},
  {"id":"mot01FreeCodeCampDemoprojects2023","abstract":"Example certification projects for our programming curriculum","accessed":{"date-parts":[[2023,4,19]]},"author":[{"literal":"moT01"},{"literal":"ojeytonwilliams"},{"literal":"scissorsneedfoodtoo"},{"literal":"naomi-lgbt"},{"literal":"ieahleen"},{"literal":"SaintPeter"},{"literal":"raisedadead"},{"literal":"RandellDawson"},{"literal":"gikf"},{"literal":"ShaunSHamilton"},{"literal":"Chris6077"},{"literal":"c-ehrlich"},{"literal":"Dave2188"},{"literal":"lasjorg"}],"citation-key":"mot01FreeCodeCampDemoprojects2023","genre":"JavaScript","issued":{"date-parts":[[2023,4,14]]},"original-date":{"date-parts":[[2020,9,20]]},"publisher":"freeCodeCamp.org","source":"GitHub","title":"freeCodeCamp/demo-projects","type":"software","URL":"https://github.com/freeCodeCamp/demo-projects"},
  {"id":"serpro69Serpro69Kotlinfaker2023","abstract":"https://serpro69.github.io/kotlin-faker/ Generate realistically looking fake data such as names, addresses, banking details, and many more, that can be used for testing and data anonymization purposes.","accessed":{"date-parts":[[2023,4,19]]},"author":[{"literal":"serpro69"},{"literal":"Grisu118"},{"literal":"scorsi"},{"literal":"Arsennikum"},{"literal":"andrew-kirkham"},{"literal":"cvk77"},{"literal":"humblehacker"},{"literal":"johanvergeer"},{"literal":"jgiovaresco"},{"literal":"el-qq"},{"literal":"Rafal-Spryszynski-Allegro"},{"literal":"TZanke"},{"literal":"urosjarc"},{"literal":"web-devel"},{"literal":"cosmin-marginean"}],"citation-key":"serpro69Serpro69Kotlinfaker2023","genre":"Kotlin","issued":{"date-parts":[[2023,4,12]]},"license":"MIT","original-date":{"date-parts":[[2019,3,11]]},"source":"GitHub","title":"serpro69/kotlin-faker","type":"software","URL":"https://github.com/serpro69/kotlin-faker"},
  {"id":"settTurbocommit2023","abstract":"turbocommit is a Rust-based CLI tool that generates high-quality git commit messages in accordance with the Conventional Commits specification, using OpenAI's `gpt-3.5-turbo` language model. It is easy to use and a cost-effective way to keep git commit history at a higher quality, helping developers stay on track with their work.","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Sett","given":""}],"citation-key":"settTurbocommit2023","genre":"Rust","issued":{"date-parts":[[2023,3,31]]},"license":"MIT","original-date":{"date-parts":[[2023,3,5]]},"source":"GitHub","title":"turbocommit","type":"software","URL":"https://github.com/Sett17/turboCommit"},
  {"id":"shadcnShadcnTaxonomy2023","abstract":"An open source application built using the new router, server components and everything new in Next.js 13.","accessed":{"date-parts":[[2023,4,19]]},"author":[{"literal":"shadcn"},{"literal":"marmorse"},{"literal":"Geczy"},{"literal":"nawok"},{"literal":"sijan2"},{"literal":"codingcodax"},{"literal":"avitorio"},{"literal":"ellisio"},{"literal":"tatchi"},{"literal":"Palanikannan1437"}],"citation-key":"shadcnShadcnTaxonomy2023","genre":"TypeScript","issued":{"date-parts":[[2023,4,19]]},"license":"MIT","original-date":{"date-parts":[[2022,10,18]]},"source":"GitHub","title":"shadcn/taxonomy","type":"software","URL":"https://github.com/shadcn/taxonomy"},
  {"id":"vaswaniAttentionAllYou2017","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","accessed":{"date-parts":[[2023,4,16]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N."},{"family":"Kaiser","given":"Lukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2017","DOI":"10.48550/arXiv.1706.03762","issued":{"date-parts":[[2017,12,5]]},"number":"arXiv:1706.03762","publisher":"arXiv","source":"arXiv.org","title":"Attention Is All You Need","type":"article","URL":"http://arxiv.org/abs/1706.03762"}
]
