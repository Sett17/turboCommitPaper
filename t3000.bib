@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2023-04-16},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\sett\\Zotero\\storage\\EKQISA8B\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;C\:\\Users\\sett\\Zotero\\storage\\J7WI8H2F\\2005.html}
}

@misc{ConventionalCommits,
  title = {Conventional {{Commits}}},
  journal = {Conventional Commits},
  urldate = {2023-04-16},
  abstract = {A specification for adding human and machine readable meaning to commit messages},
  howpublished = {https://www.conventionalcommits.org/en/v1.0.0/}
}

@misc{dcyouDcyouResume2022,
  title = {Dcyou/Resume},
  shorttitle = {Resume},
  author = {{dcyou} and {chapeupreto}},
  year = {2022},
  month = nov,
  abstract = {My resume by using NuxtJS (VueJS), Element and some useful tools - Commits {$\cdot$} dcyou/resume},
  copyright = {MIT}
}

@misc{fleschHowWritePlain2016,
  type = {Archive},
  title = {How to {{Write Plain English}}},
  author = {Flesch, Rudolf},
  year = {2016},
  month = jul,
  urldate = {2023-04-19},
  howpublished = {https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing\_guide/writing/flesch.shtml},
  file = {C\:\\Users\\sett\\Zotero\\storage\\52YHZLZK\\flesch.html}
}

@misc{honnibalSpaCyIndustrialstrengthNatural2020,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} in {{Python}}},
  shorttitle = {{{spaCy}}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year = {2020},
  doi = {10.5281/zenodo.1212303},
  urldate = {2023-04-19},
  abstract = {ðŸ’« Industrial-strength Natural Language Processing (NLP) in Python},
  copyright = {MIT}
}

@article{huChatGPTSetsRecord2023,
  title = {{{ChatGPT}} Sets Record for Fastest-Growing User Base - Analyst Note},
  author = {Hu, Krystal},
  year = {2023},
  month = feb,
  journal = {Reuters},
  urldate = {2023-04-02},
  abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
  chapter = {Technology},
  langid = {english}
}

@misc{IntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  urldate = {2023-04-02},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  howpublished = {https://openai.com/blog/chatgpt},
  langid = {american},
  file = {C\:\\Users\\sett\\Zotero\\storage\\GFAIWWQ3\\chatgpt.html}
}

@techreport{kincaidDerivationNewReadability1975,
  title = {Derivation of {{New Readability Formulas}} ({{Automated Readability Index}}, {{Fog Count}} and {{Flesch Reading Ease Formula}}) for {{Navy Enlisted Personnel}}},
  author = {Kincaid, J. Peter and Fishburne, Robert P. and Roger, Richard L. and Chissom, Brad S.},
  year = {1975},
  month = feb,
  urldate = {2023-04-28},
  abstract = {Three readability formulas were recalculated to be more suitable for Navy use. The three formulas are the Automated Readability Index ARI, Fog Count, and Flesch Reading Ease Formula. They were derived from test results of 531 Navy enlisted personnel enrolled in four technical training schools. Personnel were tested for their reading comprehension level according to the comprehension section of the Gates-McGinitie reading test. At the same time, they were tested for their comprehension of 18 passages taken from Rate Training Manuals. Scores on the reading test and training material passages allowed the calculation of the grade level of the passages. This scaled reading grade level is based on Navy personnel reading Navy training material and comprehending it.},
  chapter = {Technical Reports},
  langid = {english},
  file = {C\:\\Users\\sett\\Zotero\\storage\\IXQ3KAHX\\ADA006655.html}
}

@misc{LinguisticFeaturesSpaCy,
  title = {Linguistic {{Features}} - {{spaCy Usage Documentation}}},
  journal = {Linguistic Features},
  urldate = {2023-04-19},
  abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
  howpublished = {https://spacy.io/usage/linguistic-features\#pos-tagging},
  langid = {english},
  file = {C\:\\Users\\sett\\Zotero\\storage\\FGXDD925\\linguistic-features.html}
}

@misc{mot01FreeCodeCampDemoprojects2023,
  title = {{{freeCodeCamp}}/Demo-Projects},
  author = {{moT01} and {ojeytonwilliams} and {scissorsneedfoodtoo} and {naomi-lgbt} and {ieahleen} and {SaintPeter} and {raisedadead} and {RandellDawson} and {gikf} and {ShaunSHamilton} and {Chris6077} and {c-ehrlich} and {Dave2188} and {lasjorg}},
  year = {2023},
  month = apr,
  urldate = {2023-04-19},
  abstract = {Example certification projects for our programming curriculum},
  howpublished = {freeCodeCamp.org},
  keywords = {demo,freecodecamp,freecodecamp-project}
}

@misc{serpro69Serpro69Kotlinfaker2023,
  title = {Serpro69/Kotlin-Faker},
  author = {{serpro69} and {Grisu118} and {scorsi} and {Arsennikum} and {andrew-kirkham} and {cvk77} and {humblehacker} and {johanvergeer} and {jgiovaresco} and {el-qq} and {Rafal-Spryszynski-Allegro} and {TZanke} and {urosjarc} and {web-devel} and {cosmin-marginean}},
  year = {2023},
  month = apr,
  urldate = {2023-04-19},
  abstract = {https://serpro69.github.io/kotlin-faker/ Generate realistically looking fake data such as names, addresses, banking details, and many more, that can be used for testing and data anonymization purposes.},
  copyright = {MIT},
  keywords = {android,android-development,android-testing,anonymisation,anonymization,anonymizer,data,faker,faker-gem,faker-generator,faker-library,faker-libs,java,jvm,kotlin,kotlin-faker,kotlin-library,test-automation,testing,testing-tools}
}

@misc{settTurbocommit2023,
  title = {Turbocommit},
  author = {Sett},
  year = {2023},
  month = mar,
  urldate = {2023-03-31},
  abstract = {turbocommit is a Rust-based CLI tool that generates high-quality git commit messages in accordance with the Conventional Commits specification, using OpenAI's `gpt-3.5-turbo` language model. It is easy to use and a cost-effective way to keep git commit history at a higher quality, helping developers stay on track with their work.},
  copyright = {MIT}
}

@misc{shadcnShadcnTaxonomy2023,
  title = {Shadcn/Taxonomy},
  author = {{shadcn} and {marmorse} and {Geczy} and {nawok} and {sijan2} and {codingcodax} and {avitorio} and {ellisio} and {tatchi} and {Palanikannan1437}},
  year = {2023},
  month = apr,
  urldate = {2023-04-19},
  abstract = {An open source application built using the new router, server components and everything new in Next.js 13.},
  copyright = {MIT},
  keywords = {nextauthjs,nextjs,prisma,radix-ui,tailwindcss,typescript,vercel}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-04-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\sett\\Zotero\\storage\\GKREYTFD\\Vaswani et al. - 2017 - Attention Is All You Need.pdf;C\:\\Users\\sett\\Zotero\\storage\\MHVYTHFU\\1706.html}
}
